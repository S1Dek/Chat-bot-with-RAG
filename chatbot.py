from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_core.prompts import PromptTemplate

# ≈öcie≈ºka do lokalnej bazy wektor√≥w
CHROMA_PATH = "chroma_db"

# Tworzymy lokalny model LLM z Ollamy
llm = Ollama(model="llama3")

# Ten sam model embedding√≥w co w ingest_database.py
embeddings_model = OllamaEmbeddings(model="llama3")

# ≈ÅƒÖczymy siƒô z bazƒÖ Chroma
vector_store = Chroma(
    collection_name="local_collection",
    embedding_function=embeddings_model,
    persist_directory=CHROMA_PATH,
)

# Retriever (wyszukiwanie podobnych fragment√≥w)
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# Prompt, kt√≥ry podpowiada modelowi, jak ma odpowiadaƒá
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "Odpowiedz na pytanie u≈ºytkownika na podstawie poni≈ºszej wiedzy.\n"
        "Je≈õli odpowied≈∫ nie znajduje siƒô w wiedzy, napisz 'Nie wiem'.\n\n"
        "Wiedza:\n{context}\n\n"
        "Pytanie: {question}\n"
    ),
)

# Tworzymy ≈Ça≈Ñcuch RAG (Retrieval-Augmented Generation)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
)

print("ü§ñ Chatbot dzia≈Ça! Mo≈ºesz zadawaƒá pytania o tre≈õƒá swoich dokument√≥w.\n")

while True:
    question = input("‚ùì Pytanie: ")
    if question.lower() in ["exit", "quit", "q"]:
        print("üëã Do zobaczenia!")
        break
    answer = qa_chain.run(question)
    print("üí¨ Odpowied≈∫:", answer, "\n")
